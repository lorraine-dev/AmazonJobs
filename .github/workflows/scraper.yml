name: Amazon Jobs Scraper

on:
  schedule:
    # Run at 8am, 2pm, and 6pm UTC (3x daily)
    - cron: '0 8,14,18 * * *'
  workflow_dispatch:  # Allow manual trigger

permissions:
  contents: read
  pages: write
  id-token: write

jobs:
  scrape-and-deploy:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install Chrome
      run: |
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable

    - name: Install dependencies
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt

    - name: Download previous data
      uses: dawidd6/action-download-artifact@v3
      with:
        name: job-data
        path: data/raw/
        workflow: scraper.yml
        workflow_conclusion: success
      continue-on-error: true

    - name: Download previous state
      uses: dawidd6/action-download-artifact@v3
      with:
        name: job-state
        path: .
        workflow: scraper.yml
        workflow_conclusion: success
      continue-on-error: true

    - name: Debug downloaded files
      run: |
        echo "=== Checking downloaded files ==="
        ls -la data/raw/ || echo "data/raw/ directory not found"
        find . -name "*.csv" -type f || echo "No CSV files found"

    - name: Check TheirStack secret presence
      env:
        THEIR_STACK_API_KEY: ${{ secrets.THEIR_STACK_API_KEY }}
      run: |
        if [ -z "${THEIR_STACK_API_KEY}" ]; then
          echo "‚ùå THEIR_STACK_API_KEY is not set"; exit 1
        else
          echo "‚úÖ TheirStack key detected (masked)"
        fi

    - name: Run scraper with timeout
      id: scraper
      env:
        THEIR_STACK_API_KEY: ${{ secrets.THEIR_STACK_API_KEY }}
      run: |
        timeout 1800 python src/scripts/run_scraper.py  # 30 minute timeout
      continue-on-error: true

    - name: Check scraper success
      run: |
        if [ "${{ steps.scraper.outcome }}" == "success" ]; then
          echo "‚úÖ Scraper completed successfully"
        else
          echo "‚ùå Scraper failed or timed out"
          exit 1
        fi

    - name: Upload job data
      uses: actions/upload-artifact@v4
      with:
        name: job-data
        path: data/raw/
        retention-days: 30

    - name: Upload scraper state
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: job-state
        path: theirstack_state.json
        retention-days: 30
        if-no-files-found: ignore

    - name: Upload backups (optional)
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: job-backups
        path: data/backups/
        retention-days: 7
        if-no-files-found: ignore

    - name: Generate dashboard
      run: |
        python -m src.utils.data_processor

    - name: Setup Pages
      uses: actions/configure-pages@v4

    - name: Upload artifact
      uses: actions/upload-pages-artifact@v3
      with:
        path: ./docs

    - name: Deploy to GitHub Pages
      id: deployment
      uses: actions/deploy-pages@v4

    - name: Notify on success
      if: success()
      run: |
        echo "‚úÖ Amazon Jobs Scraper completed successfully"
        echo "üìä Dashboard deployed to: https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}"
        echo "üìÖ Run completed at: $(date)"

    - name: Notify on failure
      if: failure()
      run: |
        echo "‚ùå Amazon Jobs Scraper failed"
        echo "üìÖ Failure occurred at: $(date)"
        echo "üîç Check the logs above for details"
