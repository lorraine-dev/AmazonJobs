name: Amazon Jobs Scraper

on:
  schedule:
    # Run at 8am, 2pm, and 6pm UTC (3x daily)
    - cron: '0 8,14,18 * * *'
  workflow_dispatch:
    inputs:
      amazon_engine:
        description: 'Amazon engine (api|selenium)'
        required: false
        default: 'api'

permissions:
  contents: read
  pages: write
  id-token: write

env:
  # Use workflow input when manually dispatched; default to 'api' on schedule runs
  AMAZON_ENGINE: ${{ github.event_name == 'workflow_dispatch' && inputs.amazon_engine || 'api' }}

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  scrape-and-deploy:
    runs-on: ubuntu-latest
    timeout-minutes: 40

    steps:
    - name: Checkout repository
      uses: actions/checkout@08eba0b27e820071cde6df949e0beb9ba4906955 # v4

    - name: Set up Python
      uses: actions/setup-python@7f4fc3e22c37d6ff65e88745f38bd3157c663f7c # v4
      with:
        python-version: '3.11'
        cache: 'pip'
        cache-dependency-path: requirements.txt

    - name: Install Chrome
      if: ${{ env.AMAZON_ENGINE == 'selenium' }}
      run: |
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable

    - name: Install dependencies
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt
        if [ "$AMAZON_ENGINE" = "selenium" ]; then
          echo "Installing selenium optional dependencies..."
          pip install '.[selenium]'
        fi

    - name: Download previous data
      uses: dawidd6/action-download-artifact@09f2f74827fd3a8607589e5ad7f9398816f540fe # v3
      with:
        name: job-data
        path: data/raw/
        workflow: scraper.yml
        workflow_conclusion: success
      continue-on-error: true

    - name: Download previous state
      uses: dawidd6/action-download-artifact@09f2f74827fd3a8607589e5ad7f9398816f540fe # v3
      with:
        name: job-state
        path: .
        workflow: scraper.yml
        workflow_conclusion: success
      continue-on-error: true

    - name: Debug downloaded files
      run: |
        echo "=== Checking downloaded files ==="
        ls -la data/raw/ || echo "data/raw/ directory not found"
        find . -name "*.csv" -type f || echo "No CSV files found"

    - name: Check TheirStack secret presence
      env:
        THEIR_STACK_API_KEY: ${{ secrets.THEIR_STACK_API_KEY }}
      run: |
        if [ -z "${THEIR_STACK_API_KEY}" ]; then
          echo "‚ùå THEIR_STACK_API_KEY is not set"; exit 1
        else
          echo "‚úÖ TheirStack key detected (masked)"
        fi

    - name: Run scraper with timeout
      id: scraper
      env:
        THEIR_STACK_API_KEY: ${{ secrets.THEIR_STACK_API_KEY }}
      run: |
        timeout 1800 python src/scripts/run_scraper.py --amazon-engine "$AMAZON_ENGINE"  # 30 minute timeout
      continue-on-error: true

    - name: Print scraper log tail
      if: always()
      run: |
        echo "=== Tail logs/scraper.log (if present) ==="
        if [ -f logs/scraper.log ]; then
          tail -n 200 logs/scraper.log || true
        else
          echo "logs/scraper.log not found"
        fi

    - name: Check scraper success
      run: |
        if [ "${{ steps.scraper.outcome }}" == "success" ]; then
          echo "‚úÖ Scraper completed successfully"
        else
          echo "‚ùå Scraper failed or timed out"
          exit 1
        fi

    - name: Upload job data
      if: always()
      uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4
      with:
        name: job-data
        path: data/raw/
        retention-days: 30

    - name: Upload scraper logs
      if: always()
      uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4
      with:
        name: logs
        path: logs/
        retention-days: 14
        if-no-files-found: ignore

    - name: Upload scraper state
      if: always()
      uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4
      with:
        name: job-state
        path: theirstack_state.json
        retention-days: 30
        if-no-files-found: ignore

    - name: Upload backups (optional)
      if: always()
      uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4
      with:
        name: job-backups
        path: data/backups/
        retention-days: 7
        if-no-files-found: ignore

    - name: Generate dashboard
      run: |
        python -m src.utils.data_processor

    - name: Setup Pages
      uses: actions/configure-pages@1f0c5cde4bc74cd7e1254d0cb4de8d49e9068c7d # v4

    - name: Upload artifact
      uses: actions/upload-pages-artifact@56afc609e74202658d3ffba0e8f6dda462b719fa # v3
      with:
        path: ./docs

    - name: Deploy to GitHub Pages
      id: deployment
      uses: actions/deploy-pages@d6db90164ac5ed86f2b6aed7e0febac5b3c0c03e # v4

    - name: Notify on success
      if: success()
      run: |
        echo "‚úÖ Amazon Jobs Scraper completed successfully"
        echo "üìä Dashboard deployed to: https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}"
        echo "üìÖ Run completed at: $(date)"

    - name: Notify on failure
      if: failure()
      run: |
        echo "‚ùå Amazon Jobs Scraper failed"
        echo "üìÖ Failure occurred at: $(date)"
        echo "üîç Check the logs above for details"
