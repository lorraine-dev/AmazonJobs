name: Amazon Jobs Scraper

on:
  schedule:
    # Run at 8am, and 2pm UTC (adjust for your timezone)
    - cron: '0 8,14 * * *'
  workflow_dispatch:  # Allow manual trigger

permissions:
  contents: read
  pages: write
  id-token: write

jobs:
  scrape-and-deploy:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install Chrome
      run: |
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable
        
    - name: Install dependencies
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Download previous data
      uses: dawidd6/action-download-artifact@v3
      with:
        name: job-data
        path: data/raw/
        workflow: scraper.yml
        workflow_conclusion: success
      continue-on-error: true
        
    - name: Debug downloaded files
      run: |
        echo "=== Checking downloaded files ==="
        ls -la data/raw/ || echo "data/raw/ directory not found"
        find . -name "*.csv" -type f || echo "No CSV files found"
        
    - name: Run scraper
      run: |
        python src/scripts/run_scraper.py
        
    - name: Upload job data
      uses: actions/upload-artifact@v4
      with:
        name: job-data
        path: data/raw/
        retention-days: 30
        
    - name: Generate dashboard
      run: |
        python -m src.utils.data_processor
        
    - name: Setup Pages
      uses: actions/configure-pages@v4
      
    - name: Upload artifact
      uses: actions/upload-pages-artifact@v3
      with:
        path: ./docs
        
    - name: Deploy to GitHub Pages
      id: deployment
      uses: actions/deploy-pages@v4 